import pandas as pd
import xgboost as xgb
import numpy as np
import yaml
import sys
import os
import joblib
import optuna  # <--- ÎÎ•Î‘ Î Î¡ÎŸÎ£Î˜Î—ÎšÎ—
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from src.logger_setup import setup_logger
from src.validator import validate_input_file

# Setup Logger
try:
    logger = setup_logger()
except RuntimeError as e:
    print(f"CRITICAL: Logger setup failed: {e}")
    sys.exit(1)

def load_config() -> dict:
    config_path = "config/config.yaml"
    try:
        with open(config_path, 'r', encoding='utf-8') as file:
            config = yaml.safe_load(file)
        logger.info(f"Config loaded from {config_path}")
        return config
    except Exception as e:
        logger.error(f"Failed to load config: {e}")
        sys.exit(1)

def objective(trial, X, y):
    """
    Î— ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· Ï€Î¿Ï… ÎºÎ±Î»ÎµÎ¯ Î· Optuna Î³Î¹Î± Î½Î± Î²Î±Î¸Î¼Î¿Î»Î¿Î³Î®ÏƒÎµÎ¹ Î­Î½Î±Î½ ÏƒÏ…Î½Î´Ï…Î±ÏƒÎ¼ÏŒ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½.
    """
    # 1. ÎŸÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ "Î•ÏÏÎ¿Ï‚ Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚" (Search Space)
    params = {
        'objective': 'reg:squarederror',
        'n_jobs': -1,
        'random_state': 42,
        # Î— Optuna Î´Î¹Î±Î»Î­Î³ÎµÎ¹ Ï„Î¹Î¼Î­Ï‚ Î±Ï€ÏŒ ÎµÎ´Ï:
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2), # ÎÎµÎºÎ¹Î½Î¬Î¼Îµ Î¼Îµ "Î³ÏÎ®Î³Î¿ÏÎ¿" LR
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),  # L1 Regularization
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10) # L2 Regularization
    }

    # 2. Î¤ÏÎ­Ï‡Î¿Ï…Î¼Îµ Cross-Validation Î³Î¹Î± Î½Î± Î´Î¿ÏÎ¼Îµ Ï€ÏŒÏƒÎ¿ ÎºÎ±Î»Î¬ Ï„Î± Ï€Î¬ÎµÎ¹
    dtrain = xgb.DMatrix(X, label=y)
    
    cv_results = xgb.cv(
        params,
        dtrain,
        num_boost_round=1000,
        nfold=3,                    # 3-Fold Î³Î¹Î± Ï„Î±Ï‡ÏÏ„Î·Ï„Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·
        metrics='rmse',
        early_stopping_rounds=50,
        seed=42,
        verbose_eval=False
    )
    
    # Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ RMSE Ï€Î¿Ï… Ï€Î­Ï„Ï…Ï‡Îµ Î±Ï…Ï„ÏŒÏ‚ Î¿ ÏƒÏ…Î½Î´Ï…Î±ÏƒÎ¼ÏŒÏ‚
    return cv_results['test-rmse-mean'].min()

def train_model() -> None:
    logger.info("Starting model training (Hybrid Pro Approach)...")
    config = load_config()

    # --- LOADING & CLEANING (Î™Î”Î™ÎŸ ÎœÎ• Î Î¡Î™Î) ---
    raw_data_path = config['data']['raw_path']
    validate_input_file(raw_data_path)
    df = pd.read_csv(raw_data_path, encoding='utf-8')

    df['ocean_proximity'].replace(' ', '_', regex = True, inplace=True)
    
    # Î§ÎµÎ¹ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎºÎµÎ½ÏÎ½ (ÏŒÏ€Ï‰Ï‚ Ï„Î¿Î½ ÎµÎ¯Ï‡ÎµÏ‚)
    for col in df.columns:
        if len(df.loc[df[col] == '']) > 0:
            df.loc[df[col] == '', col] = 0
            df[col] = pd.to_numeric(df[col], errors='coerce')

    X = df.drop(columns=[config['model']['target']], axis=1)
    y = df[config['model']['target']]

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=config['model']['test_size'], 
        random_state=config['model']['random_state']
    )

    # Encoding (Manual - ÎŒÏ€Ï‰Ï‚ Ï„Î¿ ÎµÎ¯Ï‡ÎµÏ‚)
    logger.info("Encoding categorical variables...")
    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
    encoder.fit(X_train[['ocean_proximity']])
    
    encoded_train = encoder.transform(X_train[['ocean_proximity']])
    encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(['ocean_proximity']), index=X_train.index)
    X_train = pd.concat([X_train.drop('ocean_proximity', axis=1), encoded_train_df], axis=1)
    
    encoded_test = encoder.transform(X_test[['ocean_proximity']])
    encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(['ocean_proximity']), index=X_test.index)
    X_test = pd.concat([X_test.drop('ocean_proximity', axis=1), encoded_test_df], axis=1)

    logger.info("Preprocessing done. Starting Optimization Phases.")

    # --------------------------------------------------------------------------------------------------
    # Î¦Î‘Î£Î— 1: OPTUNA (Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î· Î´Î¿Î¼Î® Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…)
    # --------------------------------------------------------------------------------------------------
    logger.info("ğŸ§  PHASE 1: Searching for best structure with Optuna...")
    
    study = optuna.create_study(direction='minimize')
    # Î¤ÏÎ­Ï‡Î¿Ï…Î¼Îµ 20 Î´Î¿ÎºÎ¹Î¼Î­Ï‚ (Trials). ÎœÏ€Î¿ÏÎµÎ¯Ï‚ Î½Î± Ï„Î¿ Î±Ï…Î¾Î®ÏƒÎµÎ¹Ï‚ ÏƒÎµ 50 Î±Î½ Î­Ï‡ÎµÎ¹Ï‚ Ï‡ÏÏŒÎ½Î¿.
    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=20)
    
    best_params = study.best_params
    logger.info(f"âœ¨ Phase 1 Complete. Best Params: {best_params}")

    # --------------------------------------------------------------------------------------------------
    # Î¦Î‘Î£Î— 2: REFINEMENT (Î¡Î±Ï†Î¹Î½Î¬ÏÎ¹ÏƒÎ¼Î± Î¼Îµ Ï‡Î±Î¼Î·Î»ÏŒ Learning Rate)
    # --------------------------------------------------------------------------------------------------
    logger.info("ğŸ’ PHASE 2: Refining with Low Learning Rate (0.01)...")
    
    # 1. Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ ÎºÎ±Î»Î­Ï‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚
    final_params = best_params.copy()
    final_params['objective'] = 'reg:squarederror'
    final_params['n_jobs'] = -1
    
    # 2. Î•Ï†Î±ÏÎ¼ÏŒÎ¶Î¿Ï…Î¼Îµ Ï„Î¿Î½ "Î§ÏÏ…ÏƒÏŒ ÎšÎ±Î½ÏŒÎ½Î±": Î§Î±Î¼Î·Î»ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ Learning Rate
    final_params['learning_rate'] = 0.01 
    
    # 3. ÎÎ±Î½Î±Ï„ÏÎ­Ï‡Î¿Ï…Î¼Îµ CV Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î± ÎÎ•Î‘ Î´Î­Î½Ï„ÏÎ± (Î¸Î± ÎµÎ¯Î½Î±Î¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± Ï„ÏÏÎ±)
    dtrain = xgb.DMatrix(X_train, label=y_train)
    
    logger.info("â³ Calculating optimal trees for slow learning rate...")
    cv_results = xgb.cv(
        final_params,
        dtrain,
        num_boost_round=5000,       # Î”Î¯Î½Î¿Ï…Î¼Îµ Î¼ÎµÎ³Î¬Î»Î¿ Ï€ÎµÏÎ¹Î¸ÏÏÎ¹Î¿
        nfold=5,                    # Î•Î´Ï ÎºÎ¬Î½Î¿Ï…Î¼Îµ 5-fold Î³Î¹Î± Î¼Î­Î³Î¹ÏƒÏ„Î· Î±Î¾Î¹Î¿Ï€Î¹ÏƒÏ„Î¯Î±
        metrics='rmse',
        early_stopping_rounds=50,
        seed=42,
        verbose_eval=False
    )
    
    optimal_trees = cv_results.shape[0]
    logger.info(f"âœ… Optimal Trees found for Low LR: {optimal_trees}")

    # --------------------------------------------------------------------------------------------------
    # Î¦Î‘Î£Î— 3: FINAL TRAINING
    # --------------------------------------------------------------------------------------------------
    logger.info("ğŸ‹ï¸ PHASE 3: Training Final Model...")
    
    clf_xgb = xgb.XGBRegressor(
        **final_params,
        n_estimators=optimal_trees, # Î¤Î¿ Î½Î¿ÏÎ¼ÎµÏÎ¿ Ï€Î¿Ï… Î²ÏÎ®ÎºÎ±Î¼Îµ ÏƒÏ„Î· Î¦Î¬ÏƒÎ· 2
        random_state=42
    )
    
    clf_xgb.fit(X_train, y_train)
    # --------------------------------------------------------------------------------------------------

    logger.info("Evaluating on Test Set...")
    preds = clf_xgb.predict(X_test)

    r2 = r2_score(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))

    logger.info("--- ğŸ FINAL RESULTS ---")
    logger.info(f"R2 Score: {r2:.4f}")
    logger.info(f"RMSE: ${rmse:,.2f}")
    
    # Save Model
    models_dir = "models"
    os.makedirs(models_dir, exist_ok=True)
    joblib.dump(clf_xgb, os.path.join(models_dir, "xgboost_optimized.pkl"))
    logger.info("Model saved.")

if __name__ == "__main__":
    train_model()