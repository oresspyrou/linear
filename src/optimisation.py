import xgboost as xgb
import optuna
import sys

class ModelOptimizer:
    def __init__(self, config, logger):
        """
        Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿Î½ Optimizer Î¼Îµ Ï„Î¿ config ÎºÎ±Î¹ Ï„Î¿Î½ logger.
        """
        self.config = config
        self.logger = logger

    def objective(self, trial, X, y):
        """
        Î— ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· Ï€Î¿Ï… ÎºÎ±Î»ÎµÎ¯ Î· Optuna ÏƒÎµ ÎºÎ¬Î¸Îµ 'trial' (Î´Î¿ÎºÎ¹Î¼Î®).
        """
        space = self.config['optimization']['search_space']
        
        # 2. Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ (ÏƒÏ„Î±Î¸ÎµÏÎ­Ï‚)
        params = {
            'objective': 'reg:squarederror',
            'n_jobs': -1,
            'random_state': self.config['model']['random_state'],
            'verbosity': 0  # Î“Î¹Î± Î½Î± Î¼Î·Î½ Î³ÎµÎ¼Î¯Î¶ÎµÎ¹ Ï„Î¿ log Î¼Îµ Î¼Î·Î½ÏÎ¼Î±Ï„Î± Ï„Î¿Ï… XGBoost
        }

        # 3. Î”Ï…Î½Î±Î¼Î¹ÎºÎ® ÎµÏ€Î¹Î»Î¿Î³Î® Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ (Ï„Î¿ "ÎˆÎ¾Ï…Ï€Î½Î¿ Loop" Ï€Î¿Ï… Ï†Ï„Î¹Î¬Î¾Î±Î¼Îµ)
        for param_name, bounds in space.items():
            # Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ Ï„Î± ÏŒÏÎ¹Î± ÎµÎ¯Î½Î±Î¹ int Î® float
            is_int = isinstance(bounds['low'], int) and isinstance(bounds['high'], int)
            
            if is_int:
                # Î‘Î½ ÎµÎ¯Î½Î±Î¹ int (Ï€.Ï‡. max_depth), Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ suggest_int
                # Î¤Î¿ **bounds Ï€ÎµÏÎ½Î¬ÎµÎ¹ Ï„Î± low, high (ÎºÎ±Î¹ step/log Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½)
                params[param_name] = trial.suggest_int(param_name, **bounds)
            else:
                # Î‘Î½ ÎµÎ¯Î½Î±Î¹ float (Ï€.Ï‡. learning_rate), Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ suggest_float
                params[param_name] = trial.suggest_float(param_name, **bounds)

        # 4. Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Cross-Validation
        # Î¦Ï„Î¹Î¬Ï‡Î½Î¿Ï…Î¼Îµ Ï„Î¿ DMatrix (ÎµÎ¹Î´Î¹ÎºÎ® Î´Î¿Î¼Î® Ï„Î¿Ï… XGBoost Î³Î¹Î± Ï„Î±Ï‡ÏÏ„Î·Ï„Î±)
        dtrain = xgb.DMatrix(X, label=y)
        
        try:
            cv_results = xgb.cv(
                params,
                dtrain,
                num_boost_round=1000,
                nfold=3,                    # 3-Fold Î³Î¹Î± Ï„Î±Ï‡ÏÏ„Î·Ï„Î± ÏƒÏ„Î·Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·
                metrics='rmse',
                early_stopping_rounds=50,
                seed=self.config['model']['random_state'],
                verbose_eval=False          # Î”ÎµÎ½ Î¸Î­Î»Î¿Ï…Î¼Îµ prints ÏƒÎµ ÎºÎ¬Î¸Îµ Î²Î®Î¼Î±
            )
            
            # Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ (Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ¿) RMSE Ï€Î¿Ï… Î²ÏÎ­Î¸Î·ÎºÎµ
            return cv_results['test-rmse-mean'].min()
            
        except Exception as e:
            # Î‘Î½ ÎºÎ¬Ï„Î¹ ÏƒÎºÎ¬ÏƒÎµÎ¹ (Ï€.Ï‡. Ï€Î¿Î»Ï ÎºÎ±ÎºÏŒÏ‚ ÏƒÏ…Î½Î´Ï…Î±ÏƒÎ¼ÏŒÏ‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½), ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ Î­Î½Î± Ï„ÎµÏÎ¬ÏƒÏ„Î¹Î¿ Î»Î¬Î¸Î¿Ï‚
            # ÏÏƒÏ„Îµ Î· Optuna Î½Î± Î±Ï€Î¿Ï†ÏÎ³ÎµÎ¹ Î±Ï…Ï„ÏŒÎ½ Ï„Î¿Î½ Î´ÏÏŒÎ¼Î¿.
            self.logger.warning(f"Trial failed with error: {e}")
            return float('inf')

    def run_optimization(self, X, y):
        """
        Î•ÎºÎºÎ¹Î½ÎµÎ¯ Ï„Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Ï„Î·Ï‚ Optuna (Phase 1).
        """
        n_trials = self.config['optimization']['n_trials']
        self.logger.info(f"ğŸ§  PHASE 1: Starting Optuna Optimization ({n_trials} trials)...")

        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Study
        # direction='minimize' -> Î˜Î­Î»Î¿Ï…Î¼Îµ Î½Î± ÎµÎ»Î±Ï‡Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ RMSE
        study = optuna.create_study(direction='minimize')

        # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Study
        # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ lambda Î³Î¹Î± Î½Î± Ï€ÎµÏÎ¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ self.objective Î¼Îµ Ï„Î± X, y
        study.optimize(lambda trial: self.objective(trial, X, y), n_trials=n_trials)

        self.logger.info("âœ¨ Phase 1 Complete.")
        self.logger.info(f"Best Score (RMSE): {study.best_value:.4f}")
        self.logger.info(f"Best Params found: {study.best_params}")

        return study.best_params